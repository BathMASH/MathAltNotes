% -*- tex:uk -*-
% Course  MA20013
%
% uses concisenotes.tex
%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsect{Completeness}

Why do we generally do calculus in $\R$ and not in $\Q$ (the rational numbers)?

The space $\Q$ has `holes', which means that many of the usual results would not
hold. For example, the intermediate value theorem: the function $f(x) = x^3 - 2$
satisfies $f(0) = -2$ and $f(2) = 6$, but there is no $x \in \Q$ such
that $f(x) = 0$ (because $\sqrt[3]{2}$ is not rational).

Moreover, sequences in $\Q$ might have a limit in $\R\setminus \Q$, as in the case of 
$$
\left(1+\frac1n\right)^n \to e \quad \textrm{as } n\to \infty.
$$

The following concept can be used to rule out `holes' of this sort.

\begin{definition}
A sequence $(x_k)_{k \in \N}$ in a metric space $(X,d)$ is called a \emph{Cauchy
sequence} if for every $\varepsilon > 0$ there exists a number $N \in \N$ such
that for all $m,k \ge N$,
\[
d(x_m,x_k) < \varepsilon.
\]
\end{definition}

\begin{theorem} \label{thm:bounded}
Let $(x_k)_{k \in \N}$ be a Cauchy sequence in the metric
space $(X,d)$. Then the set $\{x_k: k \in \N\}$ is bounded.
\end{theorem}

\begin{remark}
If $\{x_k: k \in \N\}$ is bounded, then we speak
of a bounded sequence.
\end{remark}

\begin{proof}
There exists an $N \in \N$ such that $d(x_m,x_k) < 1$ for $m,k \ge N$. Define
\[
M = \max\{d(x_1,x_N),\ldots,d(x_{N - 1},x_N),1\}.
\]
Then for all $k \in \N$,
\[
d(x_k,x_N) \le M.
\]
So for all $m,k \in \N$,
\[
d(x_m,x_k) \le d(x_m,x_N) + d(x_k,x_N) \le 2M.
\]
We conclude that $\textrm{diam} \left(\{x_k: k \in \N\}\right) \leq 2M$.
\end{proof}

\begin{theorem} \label{thm:Cauchy}
Any convergent sequence is a Cauchy sequence.
\end{theorem}

\begin{proof}
Let $(x_k)_{k \in \N}$ be a convergent sequence in the metric space $(X,d)$.
Denote its limit by $x_0$. Fix $\varepsilon > 0$. Then there exists an $N \in \N$
such that $d(x_k,x_0) < \frac{\varepsilon}{2}$ for $k \ge N$. For $m,k \ge N$,
we then also have
\[
d(x_m,x_k) \le d(x_m,x_0) + d(x_k,x_0) < \varepsilon,
\]
as required.
\end{proof}

\begin{corollary}
Any convergent sequence in a metric space is bounded.
\end{corollary}

\begin{proof}
This follows from Theorem \ref{thm:Cauchy} and Theorem \ref{thm:bounded}.
\end{proof}

\begin{definition}
A metric space $(X,d)$ is called \emph{complete} if every Cauchy sequence in $X$ is
convergent to a limit in $X$.

A complete normed space is called a \emph{Banach} space.
\end{definition}

\np

\begin{example}
The Euclidean spaces $(\R^n, d_2)$ are complete (here $d_2 = \|\cdot \|_2$ is the Euclidean distance - and norm - defined in Example \ref{Eucl:d2}). 

Indeed, let $(x_k)_{k \in \N} \subset \R^n$ be a Cauchy sequence, with $x_k=(x_k^{(1)},\dots,x_k^{(n)})$; namely, for every $\varepsilon>0$ 
there exists $N\in \N$ such that whenever $m,k\geq N$, we have that $\|x_k -x_m\|_2<\varepsilon$.

Since $\|x_k -x_m\|_\infty \leq \|x_k -x_m\|_2$ (see, e.g. Problem Sheet 5, Homework question 2(i)), it follows that  
$$
\|x_k -x_m\|_\infty = \max_{i=1,\dots,n} |x_k^{(i)}-x_m^{(i)}| < \varepsilon.
$$
This implies in particular that for every fixed $i$ the sequence $(x_k^{(i)})$ of the $i$-th components is a Cauchy sequence in $\R$, hence convergent. 
Let $x_0^{(i)}:= \lim_k x_k^{(i)}$, and let $x_0:=(x_0^{(1)},\dots,x_0^{(n)})\in \R^n$. Since 
$$
\|x_k - x_0\|_2 \leq \sqrt n \|x_k -x_0\|_\infty = \max_{i=1,\dots,n} |x_k^{(i)}-x_0^{(i)}|,
$$
we have that $\|x_k - x_0\|_2 \to 0$ as $k\to \infty$, and so the sequence $(x_k)_{k \in \N}$ converges to $x_0$ in $d_2$.
\end{example}

\np

\begin{example}
Consider the metric subspace $\Q$ of $\R$.
Consider the sequence $(x_k)_{k \in \N}$ in $\Q$ with
\[
x_k = \max\left\{ \frac{i}{k}: i \in \N \,\textrm{ with } i^2 < 2k^2\right\}.
\]
This is the largest fraction less than $\sqrt{2}$ with denominator $k$, and
so we always have
\[
\sqrt{2} - \frac{1}{k} < x_k < \sqrt{2}.
\]
In particular, we have
\[
\left|x_m - x_k\right| < \frac{1}{N}
\]%%%% Check syntactic interpretation of verticals
when $m,k \ge N$. It follows that $(x_k)_{k \in \N}$ is a Cauchy sequence
in $\Q$. But is does not have a limit in $\Q$.
Hence $\Q$ is not complete.
\end{example}

\np

\begin{example}
Consider an arbitrary set $X \not= \emptyset$ with the discrete metric
\[
d(x,y) = \begin{cases} 0 & \text{if $x = y$}, \\ 1 & \text{if $x \not= y$}. \end{cases}
\]
We have seen: a sequence $(x_k)_{k \in \N}$ is convergent in $(X,d)$ with limit $x_0$ if there
exists an $N \in \N$ such that $x_k = x_0$ for all $k \ge N$.
What does it mean for a sequence to be a Cauchy sequence in $(X,d)$?

For any $\varepsilon > 0$, we need to have an $N \in \N$ such that $d(x_m,x_k) < \varepsilon$
for $m,k \ge N$. If $\varepsilon \le 1$, then this means $x_m = x_k$ for $m,k \ge N$.
Therefore a Cauchy sequence is convergent. Thus a metric space with
the discrete metric is always complete.
\end{example}

\np

\begin{theorem} \label{thm:B}
Let $S\neq \emptyset$ and $B(S)$ the set of bounded functions $f : S \to \R$.
For $f,g \in B(S)$, let
\[
d_\infty(f,g) =\left\|f-g\right\|_\infty = \sup_{s \in S} \left|f(s) - g(s)\right|.
\]%%%% Check syntactic interpretation of verticals
Then $(B(S),d_\infty)$ is a complete metric space.
\end{theorem}

\begin{proof}
It is easy to check that $d_\infty$ is a metric on $B(S)$. 

To verify that the space is
complete, consider a Cauchy sequence $(f_k)_{k \in \N}$
in $(B(S), d_\infty)$: namely, for any given $\varepsilon > 0$, there exists an $N \in \N$ such that $d_\infty(f_m,f_k) < \varepsilon$
whenever $m,k \ge N$. By definition of $d_\infty$ this implies that for every $s\in S$
\[
\left|f_m(s) - f_k(s)\right| \le d_\infty(f_m,f_k) < \varepsilon.
\]%%%% Check syntactic interpretation of verticals
Hence $(f_k(s))_{k \in \N}$ is a Cauchy sequence in $\R$. 
\np
Since $\R$ is complete,
there exists a limit. Call the limit $f_0(s)$. Thus for every $s \in S$ we obtain
a number $f_0(s)$, and this gives rise to a function $f_0 : S \to \R$.
This is the candidate limit of $(f_k)_k$.

We claim that $f_0$ is bounded, namely $f_0\in B(S)$.

As $(f_k)_{k \in \N}$ is a Cauchy sequence, there exists $N\in \N$ such that
$d_\infty(f_m,f_k) < 1$ for $m,k \geq N$. Moreover, the function $f_N$ is bounded. So
there exists an $M \geq 0$ such that
\[
\sup_{s\in S} \left|f_N(s)\right| \le M.
\]%%%% Check syntactic interpretation of verticals
Then for all $k \geq N$ and all $s \in S$, we have
\[
|f_k(s)| = |f_k(s) - f_N(s) + f_N(s)| \leq |f_k(s) - f_N(s)| + |f_N(s)| < M + 1.
\]%%%% Please add \left and \right to all verticals, this is probably a syntactically ambiguous case
Therefore,
\[
|f_0(s)| = \lim_{k \to \infty} |f_k(s)| \le M + 1.
\]%%%% Please add \left and \right to all verticals, this is probably a syntactically ambiguous case
This proves that $f_0$ is bounded, i.e., $f_0 \in B(S)$.

Finally, we want to show that $d_\infty(f_k,f_0) = \|f_k-f_0\|_{\infty} \to 0$ as $k \to \infty$.
To this end, fix $\varepsilon > 0$. Choose $N \in \N$ such that $d_\infty(f_m,f_k) < \frac{\varepsilon}{2}$
when $m,k \geq N$. Then for any $s \in S$,
\[
|f_k(s) - f_0(s)| = \left|f_k(s) - \lim_{m \to \infty} f_m(s)\right| = \lim_{m \to \infty} |f_k(s) - f_m(s)| \le \frac{\varepsilon}{2}
\]%%%% Please add \left and \right to all verticals, this is probably a syntactically ambiguous case
whenever $k \geq N$. This yields
\[
d_\infty(f_k,f_0) = \sup_{s \in S} \left|f_k(s) - f_0(s)\right| \le \frac{\varepsilon}{2} < \varepsilon
\]%%%% Check syntactic interpretation of verticals
for all $k \geq N$, and hence the desired convergence
\end{proof}

\np

\begin{example}
The space $C^0([0,1])$ with the metric
\[
d_\infty(f,g) = \max_{t \in [0,1]} |f(t) - g(t)| \quad (=\|f-g\|_{\infty}), \quad f,g \in C^0([0,1]),
\]%%%% Please add \left and \right to all verticals, this is probably a syntactically ambiguous case
is complete. (It is actually a Banach space, since the metric can be defined by means of a norm.)

\medskip

Indeed, let $(f_k)_{k \in \N} \subset C^0([0,1])$ be a Cauchy sequence with respect to the metric $d_\infty$. Since 
$C^0([0,1]) \subset B([0,1])$ (continuous functions on a compact set are bounded), and $(B([0,1]), d_\infty)$ 
is complete by Theorem \ref{thm:B}, if follows that $(f_k)_{k \in \N}$ converges in $d_\infty$, with limit $f_0 \in B([0,1])$. 

On the other hand, since convergence in $d_\infty$ is the same as uniform convergence, and a uniform limit of 
continuous functions is continuous (by 
Theorem 1.8%%%%%%%\ref{unif:cont}
), $f_0 \in C^0([0,1])$. This proves the claim.

\end{example}


%\begin{example}[$L^2$ space]
%
%
%\end{example}

\bis

\begin{example}
The normed space $(C^1([0,1]),\|\cdot\|_{\infty})$ is not a Banach space.
\end{example}


\subsect{Continuous maps between metric spaces}

We now extend the concept of continuity to maps between metric spaces.

\begin{definition}[Continuity between metric spaces]
Let $(X,d_X)$ and $(Y,d_Y)$ be metric spaces. A map $f : X \to Y$
is \emph{continuous at a point $x_0 \in X$} if
\[
\forall\, \varepsilon > 0 \quad \exists\, \delta > 0 \,\, \textrm{such that } \, \forall \, x \in X: d_X(x,x_0) < \delta \Rightarrow d_Y(f(x),f(x_0)) < \varepsilon.
\]
If $f$ is continuous at every point of $X$, then we say that $f$ is \emph{continuous}.
\end{definition}

\bis

\begin{example}\label{continuous:primitive}
Let $(C^0([a,b]),d_\infty)$ be the metric space of continuous functions in $[a,b]$ with the uniform convergence metric, $d_\infty(f,g)=\|f-g\|_{\infty}$. Then the map 
$$
F: C^0([a,b]) \to C^0([a,b]), \quad f\mapsto F(f),\quad  F(f)(x):= \int_a^x f(t) dt
$$
is continuous.
\end{example}

\np

\begin{example}
On $\R^2$, consider three different metrics:
\begin{itemize}
\item the Euclidean metric $d_2(x,y) = \sqrt{(x_1 - y_1)^2 + (x_2 - y_2)^2}$,
\item the metric $d_1(x,y) = |x_1 - y_1| + |x_2 - y_2|$, and
\item the railway metric
\[
d_{\mathrm{R}}(x,y) = \begin{cases} d_2(x,y) & \text{if $x$, $y$, and $0$ are collinear}, \\ d_2(x,0) + d_2(y,0) & \text{else}. \end{cases}
\]
\end{itemize}
This gives rise to three metric spaces: $(\R^2,d_2)$, $(\R^2,d_1)$, and $(\R^2,d_{\mathrm{R}})$. Consider
the identity map 
$$
\id : \R^2 \to \R^2, \quad \, \id(x_1,x_2) = (x_1,x_2).
$$
The continuity of $\id$ depends on the metrics that we use.

First regard $\id$ as a map from $(\R^2,d_2)$ to $(\R^2,d_1)$. If $d_2(x,y) < \delta$, then
\[
d_1(x,y) = |x_1 - y_1| + |x_2 - y_2| < 2\delta,
\]%%%% Please add \left and \right to all verticals, this is probably a syntactically ambiguous case
which we can make arbitrarily small by choosing $\delta$ small enough. Hence
$\id : (\R^2,d_2) \to (\R^2,d_1)$ is continuous.

Similarly, we see that $\id : (\R^2,d_1) \to (\R^2,d_2)$ is continuous.

Now we regard $\id$ as a map from $(\R^2,d_2)$ to $(\R^2,d_{\mathrm{R}})$.
Consider the point $(1,0) \in \R^2$. For $\eta \in \R$, also consider the
point $(1,\eta) \in \R^2$. We have
\[
d_2((1,0),(1,\eta)) = \eta
\]
and
\[
d_{\mathrm{R}}((1,0),(1,\eta)) = 1 + \sqrt{1 + \eta^2} \quad \text{if $\eta \not= 0$}.
\]
The latter does not become small when $\eta \to 0$, but the former does.
We conclude that the map $\id: (\R^2,d_2) \to (\R^2,d_{\mathrm{R}})$ is not continuous.

On the other hand, it is possible (and not too difficult) to show that
$\id: (\R^2,d_{\mathrm{R}}) \to (\R^2,d_2)$ is continuous.
\end{example}

\np



There are also stronger versions of continuity.
\begin{definition}\label{cont:ms}
Let $(X,d_X)$ and $(Y,d_Y)$ be metric spaces. A map $f : X \to Y$
is called \emph{uniformly continuous} if
\bis\nl
$\forall\, \varepsilon > 0 \,\, \exists\, \delta > 0 \,\, \textrm{such that } \,$
\[
 \forall \, x, y \in X: d_X(x, y) < \delta \Rightarrow d_Y(f(x), f(y)) < \varepsilon.
\]
\end{definition}

\begin{definition}
Let $(X,d_X)$ and $(Y,d_Y)$ be two metric spaces. A map $f : X \to Y$ is
called \emph{Lipschitz continuous} if there exists
a number $L \ge 0$ such that
\[
d_Y(f(x_1),f(x_2)) \le Ld_X(x_1,x_2)
\]
for all $x_1,x_2 \in X$.
\end{definition}

\begin{theorem} \label{thm:Lipschitz}
Every Lipschitz continuous map is uniformly continuous.
\end{theorem}

\begin{proof}
Exercise!
\end{proof}


\begin{definition} \label{def:contraction}
Let $(X,d)$ be a metric space. A map $f : X \to X$ is called a \emph{contraction}
if there exists a number $\alpha < 1$ such that
\[
d(f(x),f(y)) \le \alpha d(x,y)
\]
for all $x,y \in X$.
\end{definition}

\np

\begin{remark}
A contraction is automatically Lipschitz continuous, but the
condition is stronger than Lipschitz continuity, because we
require that $\alpha < 1$. To understand the meaning of the name, `contraction', just note that by definition, a 
ball of radius $1$ would be mapped inside a ball of radius $\alpha<1$, namely the set shrinks under the action of $f$.
\end{remark}

\np

\begin{theorem}[Continuity of compositions]
Let $(X,d_X)$, $(Y,d_Y)$, and $(Z,d_Z)$ be metric spaces.
Suppose that $f : X \to Y$ and $g : Y \to Z$ are maps. Let $x_0 \in X$. If $f$ is continuous
at $x_0$ and $g$ is continuous at $f(x_0)$, then $g \circ f$ is continuous at $x_0$.
\end{theorem}

\begin{proof}
Fix $\varepsilon > 0$. As $g$ is continuous at $f(x_0)$, there exists a $\rho > 0$ such
that
\[
d_Y(y,f(x_0)) < \rho \Rightarrow d_Z(g(y),g(f(x_0))) < \varepsilon.
\]
As $f$ is continuous at $x_0$, there exists a $\delta > 0$ such that
\[
d_X(x,x_0) < \delta \Rightarrow d_Y(f(x),f(x_0)) < \rho.
\]
So if $d_X(x,x_0) < \delta$, it follows that
$d_Z(g(f(x)),g(f(x_0))) < \varepsilon$, as required.
\end{proof}

\np

In metric spaces, continuity can be equivalently stated in terms of sequences.


\begin{theorem}\label{th:sequential}
Let $(X,d_X)$ and $(Y,d_Y)$ be metric spaces. Let $x_0 \in X$ and let $f : X \to Y$
be a map. The following are equivalent.
\begin{enumerate}
\item The map $f$ is continuous at $x_0$.
\item If $(x_k)_{k \in \N}$ is a sequence in $X$ with $x_k \to x_0$ as $k \to \infty$,
then $f(x_k) \to f(x_0)$ as $k \to \infty$.
\end{enumerate}
\end{theorem}

\np

\begin{proof}
$(1) \Rightarrow (2)$: Suppose that $f$ is continuous at $x_0$, and let $\varepsilon > 0$. Then there exists a $\delta > 0$ such that
\[
d_X(x,x_0) < \delta \Rightarrow d_Y(f(x),f(x_0)) < \varepsilon.
\]
Now let $(x_k)_{k \in \N}$ be
a sequence with $x_k \to x_0$ as $k \to \infty$. There exists an $N \in \N$ such that $d_X(x_k,x_0) < \delta$ whenever $k \ge N$.
So for $k \ge N$, we also have $d_Y(f(x_k),f(x_0)) < \varepsilon$. It then follows
that $f(x_k) \to f(x_0)$ as $k \to \infty$.

\bis

$(2) \Rightarrow (1)$: Now suppose that $f$ is not continuous at $x_0$. That is,
\[
\exists \,\varepsilon > 0 \,\,\textrm{such that } \, \forall\, \delta > 0 \, \exists\, x \in X: d_X(x,x_0) < \delta \wedge d_Y(f(x),f(x_0)) \ge \varepsilon.
\]
Choose $\varepsilon > 0$ with this property and consider $\delta = \frac{1}{k}$ for an
arbitrary $k \in \N$. We conclude that there exists a point $x_k \in X$ with
$d_X(x_k,x_0) < \frac{1}{k}$ and $d_Y(f(x_k),f(x_0)) \ge \varepsilon$.
Since we have such a point $x_k$ for every $k \in \N$, we obtain a sequence
$(x_k)_{k \in \N}$. This sequence satisfies $x_k \to x_0$, but $f(x_k) \not\to f(x_0)$
as $k \to \infty$.
\end{proof}


\np

\begin{remark}
Note that the previous theorem and Example \ref{continuous:primitive} imply 
Theorem 1.10%%%%%%%%%\ref{limit:integral}
.
\end{remark}

\bis



Continuity can also be stated in terms of open sets.

\begin{theorem} \label{thm:continuity_and_open_sets}
Let $(X,d_X)$ and $(Y,d_Y)$ be metric spaces and $f : X \to Y$ a map.
The following are equivalent.
\begin{enumerate}
\item \label{item3.1.2.i} The map $f$ is continuous.
\item \label{item3.1.2.ii} If $U \subset Y$ is open in $Y$, then
\[
f^{-1}(U) = \{x \in X: f(x) \in U\}
\]
is open in $X$.
\item \label{item3.1.2.iii} If $F \subset Y$ is closed in $Y$, then $f^{-1}(F)$ is closed in $X$.
\end{enumerate}
\end{theorem}

\begin{proof}
We use the notation $B_r^X(x_0)$ and $B_r^Y(y_0)$ for open balls in $X$ and $Y$, respectively.

Firstly we show that the statements \eqref{item3.1.2.i} and \eqref{item3.1.2.ii} are equivalent.

Suppose that $f$ is continuous, and let $U \subset Y$ be open. Let $x_0 \in f^{-1}(U)$.
Since $U$ is open, and $f(x_0) \in U$, there exists an $\varepsilon > 0$ such that $B_\varepsilon^Y(f(x_0)) \subset U$.
Choose a corresponding $\delta > 0$ such that
\[
d_X(x,x_0) < \delta \Rightarrow d_Y(f(x),f(x_0)) < \varepsilon.
\]
This condition can be rewritten in the form
\[
x \in B_\delta^X(x_0) \Rightarrow f(x) \in B_\varepsilon^Y(f(x_0)).
\]
Therefore, we have $B_\delta^X(x_0) \subset f^{-1}(U)$ and $f^{-1}(U)$ is open.

Conversely, suppose that $f^{-1}(U)$ is open for every open set $U \subset Y$.
Let $x_0 \in X$ and fix $\varepsilon > 0$. Now $B_\varepsilon^Y(f(x_0))$ is an open
set in $Y$ by Lemma \ref{lemma:balls}. So $V := f^{-1}(B_\varepsilon^Y(f(x_0)))$ is open
in $X$. We have $x_0 \in V$ (because $f(x_0) \in B_\varepsilon^Y(f(x_0))$). Hence there
exists a number $\delta > 0$ such that $B_\delta^X(x_0) \subset V$.
As a consequence, we have the following: if $d_X(x,x_0) < \delta$, then
$x \in B_\delta^X(x_0) \subset V$, so $f(x) \in B_\varepsilon^Y(f(x_0))$, so
$d_Y(f(x),f(x_0)) < \varepsilon$. Thus $f$ is continuous.

\np

Next we show that \eqref{item3.1.2.ii} and \eqref{item3.1.2.iii} are equivalent.
\nl
To this end, we use the fact that for any $A \subset Y$, we have $f^{-1}(Y \backslash A) = X \backslash f^{-1}(A)$.
Moreover, by Theorem \ref{thm:complement}, a set $A \subset Y$ is closed if, and only if,
its complement $Y \backslash A$ is open; and a similar statement holds in $X$.

If \eqref{item3.1.2.ii} is true and $F \subset Y$ is closed, then $Y \backslash F$ is open.
Therefore $f^{-1}(Y \backslash F) = X \backslash f^{-1}(F)$ is open, and $f^{-1}(F)$ is closed.

Conversely, if \eqref{item3.1.2.iii} is true and $U \subset Y$ is open, then $Y \backslash U$ is
closed. Hence $f^{-1}(Y \backslash U) = X \backslash f^{-1}(U)$ is closed, and $f^{-1}(U)$ is
open.
\end{proof}


%\subsubsection
\subsect{Continuous linear maps between normed spaces}

%\begin{definition}[Linear maps]
%Let $(X,\| \cdot \|_X)$ and $(Y,\|\cdot  \|_Y)$ be normed spaces. We denote with $L(X,Y)$ the space of linear and bounded functions between $X$ and $Y$. 
%\end{definition}

Recall the definition of linear maps between vector spaces. Given vector spaces $X$ and $Y$ on $\R$, a map $f: X\to Y$ is called linear if 
for every $\lambda,\mu \in \R$ and for every $x,y \in X$, we have that 
$$
f(\lambda x+ \mu y) = \lambda f(x) + \mu f(y).
$$
If $(X,\| \cdot \|_X)$ and $(Y,\|\cdot  \|_Y)$ are normed spaces, then they are vector spaces, so we can consider linear maps between them, and continuity is 
defined as in Definition \ref{cont:ms}, since normed spaces are metric spaces.

\begin{theorem}[Continuity of linear maps% between normed spaces
]\label{cont:0}
Let $(X,\| \cdot \|_X)$ and $(Y,\|\cdot  \|_Y)$ be normed spaces and let $f : X \to Y$ be a linear map. The following are equivalent:
\begin{enumerate}
\item $f$ is continuous in $X$;
\item $f$ is continuous at $0$;
\item there exists a constant $M > 0$  such that $\forall \, x \in X$: $\|f(x)\|_Y \leq M \|x\|_X$.
\end{enumerate}
%(Namely $f$ is continuous in $X$ if and only if $f$ is continuous at $0$).
\end{theorem}

\begin{proof}
$(1)\Rightarrow (2)$: By assumption $f$ is continuous at every $x\in X$, so in particular at $x=0\in X$.

\smallskip
$(2)\Rightarrow (1)$: We use the sequential characterisation of continuity in Theorem \ref{th:sequential}. Let $x\neq 0$ and let $(x_k)_k\subset X$ be a sequence with 
$\|x_k-x\|_X \to 0$ as $k\to \infty$. This means that the sequence $y_k:= x_k-x$ (still $\subset X$) converges to $0$ in norm. So, since $f$ is continuous at $0$ by assumption and 
$\|y_k-0\|_X\to 0$, we have that $\|f(y_k)-f(0)\|_Y\to 0$. But $f$ is linear, so $f(0)=0$ and $f(y_k) = f(x_k)-f(x)$; hence $\|f(x_k)-f(x)\|_Y\to 0$, which implies that $f$ is continuous at $x$.

\bis

$(2)\Rightarrow (3)$: By Definition \ref{cont:ms}, for every $\varepsilon>0$ there exists $\delta>0$ such that $\|x\|_X<\delta \Rightarrow \|f(x)\|_Y<\varepsilon$. 
Note that it is enough to show the claim for $x\neq 0$, since $f(0)=0$. So, let $x\neq 0$, and define $y:=\frac\delta2 \frac{x}{\|x\|_X}$. Since $\|y\|_X<\delta$, by assumption 
$\|f(y)\|_Y<\varepsilon$; equivalently,
$$
\varepsilon > \|f(y)\|_Y = \frac\delta2 \frac{1}{\|x\|_X}\|f(x)\|_Y \quad  \Leftrightarrow \quad \|f(x)\|_Y \leq \frac{2\varepsilon}{\delta} \|x\|_X,
$$
hence the claim follows for $M=\frac{2\varepsilon}{\delta}$.

%We equivalently prove that $(\textrm{NOT} (3)) \Rightarrow (\textrm{NOT}(2))$. By assumption, for every $M>0$ there exists $x_M\in X$ such that 
%\begin{equation*}
%\|f(x_M)\|_Y > M \|x_M\|_X.
%\end{equation*}
%In particular this is true for $M=k$, for every $k\in \N$, namely there exists $x_k\in X$ such that 
%\begin{equation}\label{false}
%\|f(x_k)\|_Y > k \|x_k\|_X.
%\end{equation}
%Note that we can assume that $x_k\neq 0$ for every $k\in \N$, since $f(0)=0$. Hence, from \eqref{false} we have that 
%\begin{equation}\label{false2}
%\frac{1}{\|x_k\|_X} \, \|f(x_k)\|_Y > k \quad \Leftrightarrow \quad \left\|f\left(\frac{x_k}{\|x_k\|_X}\right)\right\|_Y > k,
%\end{equation}
%by the linearity of $f$. By setting $y_k:= \frac{x_k}{\|x_k\|_X}$, we have that $\|y_k\|_X=1$ for every $k$, and by \eqref{false2} $\|f(y_k)\|_Y\to \infty$ as $k\to \infty$. Finally, define 
%$$
%w_k:= \frac{y_k}{\|f(y_k)\|_Y}.
%$$
%Then 
%$$
%\|w_k\|_X  = \left\|\frac{y_k}{\|f(y_k)\|_Y}\right\|_X = \frac{\|y_k\|_X}{\|f(y_k)\|_Y} = \frac{1}{\|f(y_k)\|_Y} \to 0,
%$$
%but on the other hand 
%$$
%f(w_k) = f\left(\frac{y_k}{\|f(y_k)\|_Y}\right) = \frac{f(y_k)}{\|f(y_k)\|_Y} 
%$$
%which implies that $\|f(w_k)\|=1$, so it does not converge to zero, contradicting continuity at zero.

%\smallskip
$(3)\Rightarrow (2)$: We use the sequential characterisation of continuity in Theorem \ref{th:sequential}. Let $(x_k)\subset X$ be such that $\|x_k-0\|_X \to 0$ as $k\to \infty$. Then by assumption 
$$
\|f(x_k)\|_Y \leq M\|x_k\|_X \to 0 \quad \textrm{as } k\to \infty,
$$
namely, since $f(0)=0$, $\|f(x_k) - f(0)\|_Y \to 0$, which implies the continuity of $f$ at zero.

(Alternatively: by linearity, the assumption implies that $f$ is Lipschitz continuous, hence continuous, which would prove $(3)\Rightarrow (1)$.)
\end{proof}

\np

\begin{definition}
Let $(X,\| \cdot \|_X)$ and $(Y,\|\cdot  \|_Y)$ be normed spaces. We denote with $L(X,Y)$ the \h{space of linear and continuous functions} (also called linear operators) between $X$ and $Y$.
\end{definition}


\begin{remark}
From Theorem \ref{cont:0} it follows that the image of a bounded set under a linear and continuous operator is also bounded. Because of this property, continuous linear operators are also known as bounded operators.
\end{remark}

%\medskip

It is easy to show that $L(X,Y)$ is a real vector space. We can then define a norm on it.

%\medskip
\np

\begin{definition}%[Operator norm]
\label{op:norm}
Let $(X,\| \cdot \|_X)$ and $(Y,\|\cdot  \|_Y)$ be normed spaces and $f\in L(X,Y)$. We define the \h{operator norm} of $f$ as 
$$
\|f\|_{{\textrm{op}}} = \inf\{M\ge 0 : \|f(x)\|_Y \le M \|x\|_X \mbox{ for all } x\in X\}.
$$
% exists as the set of all such $M$ is closed, nonempty, and bounded from below).
\end{definition}

\begin{remark}
The operator norm is a norm, namely it satisfies the axioms $(N1)-(N3)$. 

Moreover, by definition,
$$
\|f(x)\|_Y \le \|f\|_{{\textrm{op}}}\, \|x\|_X \mbox{ for all } x\in X,
$$
hence the $\inf$ in Definition \ref{op:norm} is actually a minimum.
\end{remark}

\np

\begin{remark}
 It is important to bear in mind that this operator norm depends on the choice of norms for the normed vector spaces $X$ and $Y$.
\end{remark}

\begin{proposition}[Equivalent definitions of the operator norm]
The following definitions are all equivalent:
$$
{\begin{aligned}\|f\|_{{op}}&=\inf\{M\geq 0:\|f(x)\|_Y\leq M\|x\|_X{\mbox{ for all }}x\in X\}\\
&=\sup\{\|f(x)\|_Y: x\in X{\textrm{ with }}\|x\|_X\leq 1\}\\
&=\sup\{\|f(x)\|_Y: x\in X{\mbox{ with }}\|x\|_X=1\}\\
&=\sup \left\{{\frac  {\|f(x)\|_Y}{\|x\|_X}}: x\in X{\mbox{ with }}x\neq 0\right\}.
\end{aligned}}
$$
\end{proposition}
\begin{proof} In problem class.
\end{proof}
\np

\begin{example}[$\ell^2$-space]
Consider the sequence space $\ell^2$ defined by
$$
\ell^2 = \left\{ (a_k)_{k \in \N}: \; a_k \in \mathbb{R}, \; \sum_k |a_k|^2 < \infty \right\}.
$$
Now take a bounded sequence $s = (s_k)_k$, namely with $\|s\|_\infty:=\sup _k |s_k|<\infty$. 

Define an operator $T_s: (\ell^2,\|\cdot\|_2) \to (\ell^2, \|\cdot\|_2)$ as:
$$
(a_k)_k \mapsto T((a_k)_k) = (s_k a_k)_k.
$$
The operator $T_s$ is linear and continuous (bounded) with operator norm
$$
\| T_s\|_{op} = \sup_k|s_k| =\| s \|_{\infty}.
$$
\end{example}




\subsect{Contraction Mapping Theorem}

Many problems can be solved by successive approximations. For example,
consider \emph{Newton's method} for finding a zero of a function $f : \R \to \R$.
Assuming that $f$ is differentiable, we choose a point $x_1 \in \R$ and we
recursively define
\[
x_{k + 1} = x_k - \frac{f(x_k)}{f'(x_k)}, \quad k = 1,2,3,\ldots
\]
Geometrically, this amounts to approximating the graph of $f$ by
a tangent line at $(x_k,f(x_k))$ and finding its intersection with
the $x$-axis%
% (see Figure \ref{fig:Newton})
.

\iffalse\boxdiag4\else
%\begin{figure}[ht]
%\labellist
%\pinlabel $x_{k+1}$ at 135 150
%\pinlabel $x_{k}$ at 335 150
%\endlabellist
\begin{center}
\includegraphics[width=0.4\textwidth]{Newton2}
%\caption{Graphical representation of a step in Newton's method.}
%\label{fig:Newton}
\end{center}
%\end{figure}
\fi

In many cases, the sequence $(x_k)_{k \in \N}$ will converge to a zero of $f$ as $k \to \infty$.


In this section, we will see that this is an example of a more general principle,
called the \emph{Contraction Mapping Principle} or Banach Fixed Point Theorem. 

%We will state and prove the theorem in $\R^n$, but it can be stated and 
%proved in a metric space with pretty much no change.


\bis 

First of all, we define what we mean by fixed point.


\begin{definition}
Let $X\neq \emptyset$, and let $f : X \to X$ be a map. A point $x_0 \in X$ is called a \emph{fixed point}
of $f$ if $f(x_0) = x_0$.
\end{definition}

\begin{theorem}[Contraction Mapping Principle\index{Contraction Mapping Principle}] \label{thm:contraction}
Let $(X,d)$ be a complete metric space, with $X\neq \emptyset$, and let $f : X \to X$ be a contraction. Then $f$ has a unique fixed point.
\end{theorem}

\np

\begin{proof}
Choose an arbitrary point $x_1 \in X$ and define the sequence $(x_k)_{k \in \N}$
recursively as follows:
\[
x_{k + 1} = f(x_k), \quad k = 1,2,3,\ldots
\]
We claim that this is a Cauchy sequence. In order to prove the claim,
define $\delta_k = d(x_{k + 1},x_k)$. Note first
that we have a number $\alpha \in (0,1)$ such that for all $k \in \N$,
\[
\delta_{k +1} = d(x_{k + 2},x_{k + 1}) = d(f(x_{k + 1}),f(x_k)) \le \alpha d(x_{k+ 1},x_k) = \alpha \delta_k,
\]
because $f$ is a contraction. Hence
$\delta_2 \le \alpha \delta_1$,
$\delta_3 \le \alpha \delta_2 \le \alpha^2 \delta_1$,
etc., and
\[
\delta_k \le \alpha^{k - 1} \delta_1, \quad k \in \N.
\]
If $m,k \in \N$ with $m < k$, then
\begin{align*}
d(x_m,x_k) & \le \delta_m + \cdots + \delta_{k - 1} \\
& \le \left(\alpha^{m - 1} + \cdots + \alpha^{k - 2}\right) \delta_1 \\
& \le \delta_1 \sum_{i = m - 1}^\infty \alpha^i = \frac{\delta_1 \alpha^{m - 1}}{1 - \alpha}.
\end{align*}
In the last step we have used the fact that $\alpha \in (0,1)$.
For the same reason, we have
\[
\frac{\delta_1 \alpha^{m - 1}}{1 - \alpha} \to 0 \quad \text{as $m \to \infty$}.
\]
Hence $(x_k)_{k \in \N}$ is a Cauchy sequence.

Because $(X,d)$ is a complete metric space, every Cauchy sequence converges.
In particular, there exists a point $x_0 \in X$ such that $d(x_k,x_0)\to 0$ as
$k \to \infty$.

Since $f$ is a contraction, it is Lipschitz continuous. In particular
it is continuous by Theorem \ref{thm:Lipschitz}. Hence
\[
f(x_0) = f\left(\lim_{k \to \infty} x_k\right) = \lim_{k \to \infty} f(x_k) = \lim_{k \to \infty} x_{k + 1} = x_0.
\]
So $x_0$ is a fixed point of $f$.

It is a unique fixed point, because for every other fixed point $y_0 \in X$,
we have
\[
d(x_0,y_0) = d(f(x_0),f(y_0)) \le \alpha d(x_0,y_0).
\]
Because $\alpha < 1$, it follows that $d(x_0,y_0) = 0$, and so $x_0 = y_0$.
\end{proof}

\bis

\begin{remark}
The proof also shows how to find the fixed point: choose a point $x_1 \in X$,
define recursively $x_{k + 1} = f(x_k)$ for $k = 1,2,\ldots$ and find the
limit $x_0 = \lim\limits_{k \to \infty} x_k.$
\end{remark}



%%%%%%%%%%%%
\np

\begin{example}[Solving algebraic equations]
Show that the equation 
\begin{equation}\label{alg:eq}
x^7 - x^3 -21 x+5 =0
\end{equation}
has a unique solution in the interval $[0,1]$.
%\bigskip
\bis

Note that the equation \eqref{alg:eq} can be equivalently rewritten as 
\begin{equation}
x= \frac{x^7 - x^3 +5}{21};
\end{equation}
so, by defining $f:[0,1] \to \R$ as $f(x):= \frac{x^7 - x^3 +5}{21}$, the original equation \eqref{alg:eq} can be written as a fixed point problem $x=f(x)$.

In particular, there exists a unique solution to \eqref{alg:eq} if and only if there is a unique fixed point of $f$ in $[0,1]$.
\medskip

This would be a direct consequence of the Contraction Mapping Theorem, \textbf{provided} $f$ is a contraction from a complete metric space in itself, so all is left is to verify this:

\begin{itemize}
\item We prove that $f([0,1])\subset [0,1]$: Note that, for every $x\in [0,1]$ we have
$$
0< \frac{- 1 +5}{21} f(x)= \frac{x^7 - x^3 +5}{21} \leq f(x):= \frac{1+5}{21} <1,
$$
which proves the claim.

\item Clearly $([0,1],|\cdot|)$ is a complete metric space, since $[0,1]$ is a closed subset of $\R$ and $(\R,|\cdot|)$ is a complete metric space (by Problem Sheet 7, Practice question 1).

\item Finally, we need to prove that $f$ is a contraction, namely that there exists $0<\alpha<1$ such that $|f(x) - f(y)| \leq \alpha |x-y|$ for every $x,y\in [0,1]$.

To see this, we apply the Intermediate Value Theorem to $f$ (which is differentiable with continuous derivative in particular) to obtain that 
$$
f(x) - f(y) = f'(\xi)(x-y), \quad \textrm{for some }  \xi \in [x,y] \quad (\textrm{or } \xi \in [y,x] \, \textrm{if } x>y).
$$
Hence
\begin{equation}\label{intermediate}
|f(x) - f(y)| \leq |f'(\xi)| |x-y| \quad \forall\, x,y \in [0,1].
\end{equation}%%%% Please add \left and \right to all verticals, this is probably a syntactically ambiguous case
Now note that we can estimate $f'$ as we estimated $f$ previously, in $[0,1]$:
$$
 \frac{- 3}{21} \leq f'(\xi) = \frac{7x^6 - 3x^2}{21} \leq  \frac{7}{21} \quad \Rightarrow \quad \|f'\|_{\infty} = \sup_{\xi\in [0,1]} |f'(\xi)| <1.
$$
From \eqref{intermediate} it follows immediately that 
$$
|f(x) - f(y)| \leq \alpha |x-y| \quad \forall\, x,y \in [0,1], \quad \alpha = \|f'\|_{\infty} <1,
$$
hence $f$ is a contraction.
\end{itemize}
%By the Contraction Mapping Theorem there exists a unique fixed point for $f$ in $[0,1]$, as claimed.
\end{example}

\begin{remark}
Note that one could have chosen, in the previous example, a function $g:[0,1]\to \R$ as 
$$
g(x):= x^7 - x^3 - 20 x + 5,
$$
for which the equation \eqref{alg:eq} can be written as a fixed point problem as well, as $x = g(x)$.

However it is easy to see that then $g([0,1]) \nsubseteq [0,1]$, which makes $g$  not admissible.
\end{remark}

\np

\begin{example}[Unique local solutions to differential equations]
Let $f:\R^2 \to \R$ be a continuous and bounded function; namely $f: (\R^2,\|\cdot \|) \to (\R, |\cdot|)$ is continuous and there exists a constant $M>0$ such that 
$|f(x,y)|\leq M$ for every $x, y\in \R$. Assume also that there exists a constant $L>0$ such that 
\begin{equation}\label{L2}
|f(x,y)-f(x,z)| \leq L|y-z| \quad \forall \,  x, y, z\in \R.
\end{equation}%%%% Please add \left and \right to all verticals, this is probably a syntactically ambiguous case

Show that for every $(x_0,y_0) \in \R^2$ the ordinary differential equation (ODE)
\begin{equation}\label{ODE}
y'(x) = f(x,y(x)), \quad y(x_0)=y_0
\end{equation}
has a unique solution in the interval $I:= \big[x_0-\frac{1}{2L}, x_0+\frac{1}{2L}\big]$.
\bigskip

We want to write the ODE as a fixed point problem; we do it by integrating the equation between $x_0$ and $x$:
$$
y(x) - y_0 = \int_{x_0}^x y'(t) dt = \int_{x_0}^x f(t,y(t)) dt  \quad \Leftrightarrow \quad y(x) = y_0 + \int_{x_0}^x f(t,y(t)) dt.
$$
We can define for $y\in C^0(I)$ a function $F(y): I\to \R$, as $(F(y))(x):= y_0 + \int_{x_0}^x f(t,y(t)) dt$; hence the ODE becomes $y = F(y)$. 
In particular, there exists a unique solution to the ODE if and only if there is a unique fixed point of $F$ in $C^0(I)$.
\medskip

Again, this would be a direct consequence of the Contraction Mapping Theorem, \textbf{provided} $F$ is a contraction from a complete metric space into itself,  so all is left is to verify this:

\begin{itemize}
\item We prove that $F(C^0(I))\subset C^0(I)$, namely that $F(y) \in C^0(I)$ for every $y \in C^0(I)$. For $x_1,x_2\in I$, $x_1<x_2$, we have 
\begin{align*}
(F(y))(x_2) - (F(y))(x_1) &= \int_{x_0}^{x_2} f(t,y(t)) dt - \int_{x_0}^{x_1} f(t,y(t)) dt \\&= \int_{x_1}^{x_2} f(t,y(t)) dt \leq M\, \left|x_2-x_1\right|,
\end{align*}%%%% Check syntactic interpretation of verticals
and similarly if $x_1>x_2$; so $F(y)$ is a Lipschitz function, hence continuous. 

\item Clearly $(C^0(I), \|\cdot\|_{\infty})$ is a complete metric space.

\item Finally, we need to prove that $F$ is a contraction. Let $y_1, y_2 \in C^0(I)$; then for every $x\in I$
\begin{align*}
[F(y_1) - F(y_2)](x) &= \int_{x_0}^{x} f(t,y_1(t)) dt - \int_{x_0}^{x} f(t,y_2(t)) dt \\
& \leq L \int_{x_0}^x \left|y_1(t) - y_2(t)\right| dt \\&\leq L|x_0-x| \|y_1-y_2\|_{\infty} \leq \frac12 \|y_1-y_2\|_{\infty}.
\end{align*}%%%% Please add \left and \right to all verticals, this is probably a syntactically ambiguous case
So this implies that 
$$
\| F(y_1) - F(y_2)\|_{\infty} \leq \frac12 \|y_1-y_2\|_{\infty},
$$
hence $F$ is a contraction.
\end{itemize}

%Thus, by the Contraction Mapping Theorem there exists a unique fixed point for $F$ in $C^0(I)$, as claimed.


\end{example}



%%%%%%%%%%%%%




\begin{example}[Revisit the Newton method at the start of this section]
Let $f : \R \to \R$ be a function with continuous derivatives up to second order. We want to reformulate Newton's method
as an example of the Contraction Mapping Principle. Guided by the iteration
formula, we define $F : \R \to \R$ by
\[
F(x) := x - \frac{f(x)}{f'(x)}
\]
for $x \in \R$ with $f'(x) \not= 0$. Is this a contraction?

We have a criterion involving the derivative of $F$, so we compute
\[
F'(x) = 1 - \frac{f'(x)}{f'(x)} + \frac{f(x) f''(x)}{(f'(x))^2} = \frac{f(x) f''(x)}{(f'(x))^2}.
\]
This does not necessarily satisfy $|F'(x)| < 1$, and thus $F$ is not
a contraction in general. In fact, it is not even necessarily well-defined on all of
$\R$, as $f'$ may have zeros. 
\nl
On the other hand, we know that
Newton's method does not work in every situation, so it's no surprise
that we need some more restrictions.

Suppose for simplicity that we look for a simple zero $x_0 \in \R$ of
$f$, i.e., $f(x_0) = 0$ \textbf{but} $f'(x_0)\not= 0$. Then
\[
\lim_{x \to x_0}F'(x)= \lim_{x \to x_0}\frac{f(x) f''(x)}{(f'(x))^2}
% \to 0\text{\quad as\quad}
%x \to x_0
=0.
\]
Hence there exists a number $a > 0$ such that 
$$|F'(x)| \le \frac{1}{2}
\text{\quad on\quad}
[x_0 - a,x_0 + a]=:I.$$
Hence for $x, y \in I$,
we have
\[
|F(x) - F(y)| \le \frac{1}{2} |x - y|.
\]%%%% Please add \left and \right to all verticals, this is probably a syntactically ambiguous case
In particular, this means that for $x \in I$,
we have
\[
|F(x) - x_0| = |F(x) - F(x_0)| \le \frac{1}{2} |x - x_0| < a.
\]%%%% Please add \left and \right to all verticals, this is probably a syntactically ambiguous case
It follows that $F(I) \subset I$.
Hence we can define a map $G :I \to I$
with $G(x) = F(x)$ on $I$, and $G$ is a contraction.
\np
Since $I$ is a complete metric space, Theorem \ref{thm:contraction}
applies. We obtain a unique point $y_0 \in I$ with $G(y_0) = y_0$,
i.e.,
\[
y_0 = y_0 - \frac{f(y_0)}{f'(y_0)}.
\]
This implies $f(y_0) = 0$.

\bis

Of course we have started with the assumption $f(x_0) = 0$, so this outcome
is no surprise. In fact, the uniqueness of the fixed point implies $y_0 = x_0$.
As $x_0$ is unknown when we use Newton's method, we cannot determine $a$
in advance, and so in practice, we often have to take an educated guess
for the initial point. But these observations prove that under certain circumstances,
Newton's method will work (i.e.\ converge), as long as we choose a sufficiently
good approximation as initial point.
\end{example}

